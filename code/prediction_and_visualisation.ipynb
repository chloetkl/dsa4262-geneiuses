{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install nbimporter\n",
    "%pip install scikit-misc\n",
    "\n",
    "import nbimporter  \n",
    "import gzip\n",
    "import shutil\n",
    "import os\n",
    "import json \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from main import generate_predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Access SG-NEx data through AWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # list all samples that have processed data for RNA modification detection using m6Anet\n",
    "!aws s3 ls --no-sign-request s3://sg-nex-data/data/processed_data/m6Anet/  \n",
    "\n",
    "# saves all samples that have processed data for RNA modification detection using m6Anet under data directory\n",
    "!aws s3 cp --no-sign-request s3://sg-nex-data/data/processed_data/m6Anet/ ../data/sg-nex-data/raw/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Prediction of SG-NEx data using Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prediction_files(raw_directory, prediction_directory):\n",
    "    # Ensure the output directory exists\n",
    "    os.makedirs(prediction_directory, exist_ok=True)\n",
    "\n",
    "    # Iterate through each folder in the data directory\n",
    "    for root, dirs, files in os.walk(raw_directory):\n",
    "        for file in files:\n",
    "            if file.endswith('.json'):\n",
    "                input_path = os.path.join(root, file)\n",
    "                \n",
    "                # Get the base folder name from the input path\n",
    "                folder_name = os.path.basename(root)\n",
    "                \n",
    "                # Construct the output path\n",
    "                output_path = os.path.join(prediction_directory, f\"{folder_name}.csv\")\n",
    "                \n",
    "                print(f\"Processing: {input_path}\")\n",
    "                print(f\"Output will be saved to: {output_path}\")\n",
    "                \n",
    "                # Call prediction function\n",
    "                generate_predictions(input_path, '../model/selector.joblib.gz', '../model/rf_classifier.joblib.gz', output_path, include_features = True)\n",
    "\n",
    "                # Remove the folder after the CSV is successfully exported\n",
    "                shutil.rmtree(root)\n",
    "                print(f\"Removed folder: {root}\")\n",
    "\n",
    "    # Remove sg-nex-data directory \n",
    "    sgnex_directory = os.path.dirname(raw_directory)  \n",
    "    shutil.rmtree(sgnex_directory)  # This will remove 'data/' and everything inside it\n",
    "    print(f\"Removed sg-nex-data directory: {sgnex_directory}\")\n",
    "\n",
    "raw_directory = '../data/sg-nex-data/raw'\n",
    "prediction_directory = '../output'\n",
    "generate_prediction_files(raw_directory, prediction_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Directory containing the prediction CSV files\n",
    "prediction_directory = '../output'\n",
    "\n",
    "# Dictionary to store dataframes by cell line\n",
    "cell_line_dataframes = {\n",
    "    'A549': [],\n",
    "    'Hct116': [],\n",
    "    'K562': [],\n",
    "    'HepG2': [],\n",
    "    'MCF7': []\n",
    "}\n",
    "\n",
    "# Iterate through the files in the prediction directory\n",
    "for file_name in os.listdir(prediction_directory):\n",
    "    if file_name.endswith('.csv'):\n",
    "        file_path = os.path.join(prediction_directory, file_name)\n",
    "        \n",
    "        # Load the CSV file into a dataframe\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Check the file name to determine which cell line it belongs to\n",
    "        if 'A549' in file_name:\n",
    "            df['cell_line'] = 'A549'\n",
    "            cell_line_dataframes['A549'].append(df)\n",
    "        elif 'Hct116' in file_name:\n",
    "            df['cell_line'] = 'Hct116'\n",
    "            cell_line_dataframes['Hct116'].append(df)\n",
    "        elif 'K562' in file_name:\n",
    "            df['cell_line'] = 'K562'\n",
    "            cell_line_dataframes['K562'].append(df)\n",
    "        elif 'HepG2' in file_name:\n",
    "            df['cell_line'] = 'HepG2'\n",
    "            cell_line_dataframes['HepG2'].append(df)\n",
    "        elif 'MCF7' in file_name:\n",
    "            df['cell_line'] = 'MCF7'\n",
    "            cell_line_dataframes['MCF7'].append(df)\n",
    "\n",
    "# Concatenate dataframes for each cell line into a single dataframe\n",
    "combined_data = pd.concat(\n",
    "    [pd.concat(dfs) for dfs in cell_line_dataframes.values() if dfs], \n",
    "    ignore_index=True\n",
    ")\n",
    "\n",
    "print(combined_data.head())\n",
    "\n",
    "# import os\n",
    "# import pandas as pd\n",
    "\n",
    "# # Directory containing the prediction CSV files\n",
    "# prediction_directory = '../data/sg-nex-data/predictions'\n",
    "\n",
    "# # Dictionary to store dataframes by cell line\n",
    "# cell_line_dataframes = {\n",
    "#     'A549': [],\n",
    "#     'Hct116': [],\n",
    "#     'K562': [],\n",
    "#     'HepG2': [],\n",
    "#     'MCF7': []\n",
    "# }\n",
    "\n",
    "# # Iterate through the files in the prediction directory\n",
    "# for file_name in os.listdir(prediction_directory):\n",
    "#     if file_name.endswith('.csv'):\n",
    "#         file_path = os.path.join(prediction_directory, file_name)\n",
    "        \n",
    "#         # Load the CSV file into a dataframe\n",
    "#         df = pd.read_csv(file_path)\n",
    "        \n",
    "#         # Check the file name to determine which cell line it belongs to\n",
    "#         if 'A549' in file_name:\n",
    "#             cell_line_dataframes['A549'].append(df)\n",
    "#         elif 'Hct116' in file_name:\n",
    "#             cell_line_dataframes['Hct116'].append(df)\n",
    "#         elif 'K562' in file_name:\n",
    "#             cell_line_dataframes['K562'].append(df)\n",
    "#         elif 'HepG2' in file_name:\n",
    "#             cell_line_dataframes['HepG2'].append(df)\n",
    "#         elif 'MCF7' in file_name:\n",
    "#             cell_line_dataframes['MCF7'].append(df)\n",
    "\n",
    "# # Optionally, concatenate the dataframes for each cell line into a single dataframe\n",
    "# for cell_line, dfs in cell_line_dataframes.items():\n",
    "#     # Concatenate dataframes if the list is not empty\n",
    "#     cell_line_dataframes[cell_line] = pd.concat(dfs, ignore_index=True) if dfs else pd.DataFrame()\n",
    "\n",
    "# # Now, cell_line_dataframes dictionary contains concatenated dataframes for each cell line\n",
    "\n",
    "# # Example: Print the first few rows of each cell line dataframe to verify\n",
    "# for cell_line, df in cell_line_dataframes.items():\n",
    "#     print(f\"First few rows of {cell_line} dataframe:\")\n",
    "#     print(df.head())\n",
    "#     print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## Visualisations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "### Boxplot to analyse features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# List of features to plot\n",
    "features = [\n",
    "    '-1_dwelling_time_mean', '-1_dwelling_time_min', '-1_dwelling_time_max',\n",
    "    '-1_standard_dev_mean', '-1_mean_current_mean', '-1_mean_current_min',\n",
    "    '-1_mean_current_max', '0_dwelling_time_mean', '0_dwelling_time_min',\n",
    "    '0_dwelling_time_max', '0_standard_dev_mean', '0_mean_current_mean',\n",
    "    '0_mean_current_min', '0_mean_current_max', '+1_dwelling_time_mean',\n",
    "    '+1_dwelling_time_min', '+1_dwelling_time_max', '+1_standard_dev_mean',\n",
    "    '+1_mean_current_mean', '+1_mean_current_min', '+1_mean_current_max'\n",
    "]\n",
    "\n",
    "# Combine dataframes for all cell lines into a single dataframe for plotting\n",
    "combined_df = pd.DataFrame()\n",
    "for cell_line, df in cell_line_dataframes.items():\n",
    "    if 'transcript_id' in df.columns and 'prediction' in df.columns:\n",
    "        df['cell_line'] = cell_line  # Add a column for the cell line\n",
    "        # Map prediction 1 to Modified and 0 to Unmodified\n",
    "        df['Modification'] = df['prediction'].map({1: 'Modified', 0: 'Unmodified'})\n",
    "        combined_df = pd.concat([combined_df, df], ignore_index=True)\n",
    "\n",
    "# Create a combined boxplot for each feature\n",
    "for feature in features:\n",
    "    if feature not in combined_df.columns:\n",
    "        print(f\"Feature '{feature}' not found in combined dataframe.\")\n",
    "        continue\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.boxplot(\n",
    "        data=combined_df,\n",
    "        x='cell_line',\n",
    "        y=feature,\n",
    "        hue='Modification',  # Use the new Modification column for hue\n",
    "        palette='Set2'\n",
    "    )\n",
    "    plt.title(f'Boxplot of {feature} for Different Cell Lines (Modified vs Unmodified)')\n",
    "    plt.xlabel('Cell Line')\n",
    "    plt.ylabel(f'{feature}')\n",
    "    plt.legend(title='Modification')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.ylim(0, combined_df[feature].quantile(0.99))  # Limit y-axis to 99th percentile to reduce outlier effect\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "### Density Plot to analyse Positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Filter the data for scores > 0.9\n",
    "filtered_data = combined_data[combined_data['score'] > 0.9]\n",
    "\n",
    "# Set up the plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Loop through each cell line and plot the density\n",
    "for cell_line in filtered_data['cell_line'].unique():\n",
    "    sns.kdeplot(\n",
    "        data=filtered_data[filtered_data['cell_line'] == cell_line], \n",
    "        x='transcript_position', \n",
    "        label=cell_line, \n",
    "        fill=False,  # Set fill to False for lines only\n",
    "        bw_adjust=0.5  # Adjusts the smoothness of the density curve\n",
    "    )\n",
    "\n",
    "# Adding labels and title\n",
    "plt.xlabel('Transcript Position')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Density Plot of Transcript Positions for Each Cell Line (Score > 0.9)')\n",
    "plt.legend(title='Cell Line')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "### Scatterplot to analyse positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from plotnine import ggplot, aes, geom_point, geom_smooth, labs, theme_bw, theme\n",
    "\n",
    "# Filter the data for scores > 0.9 and transcript positions < 20000\n",
    "filtered_data_scatterplot = filtered_data[filtered_data['transcript_position'] < 20000]\n",
    "\n",
    "# Create individual plots for each cell line\n",
    "for cell in filtered_data_scatterplot['cell_line'].unique():\n",
    "    subset = filtered_data_scatterplot[filtered_data_scatterplot['cell_line'] == cell]\n",
    "\n",
    "    # Create the plot for the individual cell line\n",
    "    plot = (\n",
    "        ggplot(subset, aes(x='transcript_position', y='score'))\n",
    "        + geom_point(size=0.5, alpha=0.5)\n",
    "        + geom_smooth(method='loess', color='blue', span=0.3, se=True)  # Adjust span as needed\n",
    "        + labs(\n",
    "            title=f'Score by Transcript Position for Cell Line: {cell}',\n",
    "            x='Transcript Position',\n",
    "            y='Score'\n",
    "        )\n",
    "        + theme_bw()\n",
    "        + theme(figure_size=(12, 6))\n",
    "    )\n",
    "    \n",
    "    print(plot)\n",
    "\n",
    "# Combined plot for all cell lines\n",
    "overlay_plot = (\n",
    "    ggplot(filtered_data_scatterplot, aes(x='transcript_position', y='score', color='cell_line', group='cell_line'))  # Group by cell_line\n",
    "    + geom_point(size=0.5, alpha=0.3)  # Optional: Adjust alpha for visibility\n",
    "    + geom_smooth(method='loess', span=0.3, se=True)  # No need for aes() here\n",
    "    + labs(\n",
    "        title='Overlay of LOESS Smooth Lines for All Cell Lines',\n",
    "        x='Transcript Position',\n",
    "        y='Score'\n",
    "    )\n",
    "    + theme_bw()\n",
    "    + theme(figure_size=(12, 6))\n",
    ")\n",
    "\n",
    "print(overlay_plot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a combined plot for all cell lines with only LOESS smooth lines\n",
    "overlay_plot = (\n",
    "    ggplot(filtered_data_scatterplot, aes(x='transcript_position', y='score', color='cell_line', fill='cell_line', group='cell_line'))  # Group by cell_line\n",
    "    + geom_smooth(method='loess', span=0.3, se=True, alpha=0.3)  # Adjust alpha for transparency of the confidence band\n",
    "    + labs(\n",
    "        title='Overlay of LOESS Smooth Lines for All Cell Lines',\n",
    "        x='Transcript Position',\n",
    "        y='Score'\n",
    "    )\n",
    "    + theme_bw()\n",
    "    + theme(figure_size=(12, 6))\n",
    ")\n",
    "\n",
    "print(overlay_plot)\n",
    "\n",
    "# Create a combined plot for all cell lines with only LOESS smooth lines\n",
    "overlay_plot_1 = (\n",
    "    ggplot(filtered_data_scatterplot, aes(x='transcript_position', y='score', color='cell_line', fill='cell_line', group='cell_line'))  # Group by cell_line\n",
    "    + geom_smooth(method='loess', span=0.3, se=False, alpha=0.3)  # Adjust alpha for transparency of the confidence band\n",
    "    + labs(\n",
    "        title='Overlay of LOESS Smooth Lines for All Cell Lines',\n",
    "        x='Transcript Position',\n",
    "        y='Score'\n",
    "    )\n",
    "    + theme_bw()\n",
    "    + theme(figure_size=(12, 6))\n",
    ")\n",
    "\n",
    "print(overlay_plot_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count distinct transcript_ids in each cell_line\n",
    "distinct_transcripts = combined_data.groupby('cell_line')['transcript_id'].nunique().reset_index()\n",
    "\n",
    "# Rename the columns for clarity\n",
    "distinct_transcripts.columns = ['cell_line', 'distinct_transcript_count']\n",
    "\n",
    "# Display the result\n",
    "print(distinct_transcripts)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
